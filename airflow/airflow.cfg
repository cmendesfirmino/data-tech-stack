[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
# This path must be absolute
dags_folder = /usr/local/airflow/dags
# An executor is a mechanism that airflow uses to run task instances.
# There are four types of executors: SequentialExecutor, LocalExecutor,
# CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor
# The SQLAlchemy connection string to the metadata database.
# This connection string is used to connect to the database where the metadata
# database lives. Ensure you replace the placeholders with your actual values.
sql_alchemy_conn = postgresql+psycopg2://ecomm_company:ecomm_company@test_postgres:5432/company_db?options=-csearch_path=airflow_schema
# The encoding for the databases
sql_engine_encoding = utf-8
# The maximum number of tasks that can be running concurrently
parallelism = 32

[webserver]
web_server_port = 8080
web_server_worker_timeout = 120
web_server_master_timeout = 300
webserver_config = ./tmp/webserver_config.py
auth_backend = airflow.contrib.auth.backends.password_auth

[scheduler]
# Task instances are checked for their state at regular intervals and
# updated in the metadata database. This is the interval (in seconds)
# in which the job is set to run.
scheduler_heartbeat_sec = 5
# The maximum number of task instances that a specific task can try to
# execute before marking it as failed.
max_active_runs_per_dag = 16
# The maximum number of task instances allowed to run concurrently
max_active_tasks_per_dag = 16

[logging]
# Example log format
base_log_folder = /usr/local/airflow/logs
remote_logging = False
remote_log_conn_id =
google_key_path =
google_storage_bucket =
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log

