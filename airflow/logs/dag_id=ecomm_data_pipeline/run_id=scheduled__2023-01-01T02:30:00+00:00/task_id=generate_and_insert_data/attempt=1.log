[2024-06-27T22:33:36.689+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T22:33:36.700+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T22:33:36.703+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T22:33:36.703+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T22:33:36.707+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-27T22:33:36.709+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-27T22:33:36.710+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpru13def4']
[2024-06-27T22:33:36.711+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-27T22:33:36.732+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 9174587f6b1d
[2024-06-27T22:33:36.765+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-27T22:33:36.766+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T22:33:36.802+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', './generator/ecomm_generator.py']' returned non-zero exit status 2.
[2024-06-27T22:33:36.802+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T22:33:36.803+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T22:33:36.807+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240627T223336, end_date=20240627T223336
[2024-06-27T22:33:36.853+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T22:33:36.862+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T22:33:36.863+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T22:52:25.517+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T22:52:25.526+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T22:52:25.529+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T22:52:25.529+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T22:52:25.533+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-27T22:52:25.535+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-27T22:52:25.536+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp2hqwoqm2']
[2024-06-27T22:52:25.536+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-27T22:52:25.552+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host a70edf9d8581
[2024-06-27T22:52:25.580+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-27T22:52:25.581+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T22:52:25.613+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', './generator/ecomm_generator.py']' returned non-zero exit status 2.
[2024-06-27T22:52:25.613+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T22:52:25.614+0000] {logging_mixin.py:188} INFO - stderr: python3: can't open file '/opt/airflow/./generator/ecomm_generator.py': [Errno 2] No such file or directory
[2024-06-27T22:52:25.614+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T22:52:25.614+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T22:52:25.619+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240627T225225, end_date=20240627T225225
[2024-06-27T22:52:25.636+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T22:52:25.642+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T22:52:25.642+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T23:00:10.676+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T23:00:10.688+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T23:00:10.690+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T23:00:10.690+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T23:00:10.695+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-27T23:00:10.696+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-27T23:00:10.698+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp07zu51yw']
[2024-06-27T23:00:10.698+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-27T23:00:10.715+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 5cc71d4b298c
[2024-06-27T23:00:10.746+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-27T23:00:10.747+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T23:00:10.805+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T23:00:10.805+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T23:00:10.805+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T23:00:10.806+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T23:00:10.806+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T23:00:10.811+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240627T230010, end_date=20240627T230010
[2024-06-27T23:00:10.847+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T23:00:10.854+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T23:00:10.855+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T23:09:49.489+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T23:09:49.499+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T23:09:49.501+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T23:09:49.502+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T23:09:49.506+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-27T23:09:49.507+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-27T23:09:49.509+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpz3e9s_zz']
[2024-06-27T23:09:49.509+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-27T23:09:49.526+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 7147f8a4dc9f
[2024-06-27T23:09:49.560+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-27T23:09:49.561+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T23:09:49.619+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T23:09:49.619+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T23:09:49.619+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T23:09:49.620+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T23:09:49.620+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T23:09:49.625+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240627T230949, end_date=20240627T230949
[2024-06-27T23:09:49.663+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T23:09:49.671+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T23:09:49.671+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T23:13:28.000+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T23:13:28.017+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T23:13:28.023+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-27T23:13:28.023+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T23:13:28.028+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-27T23:13:28.030+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-27T23:13:28.031+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp3x8e5_av']
[2024-06-27T23:13:28.032+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-27T23:13:28.054+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 4bd55eb76a74
[2024-06-27T23:13:28.085+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-27T23:13:28.086+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T23:13:28.144+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T23:13:28.144+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T23:13:28.145+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T23:13:28.145+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T23:13:28.145+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T23:13:28.150+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240627T231328, end_date=20240627T231328
[2024-06-27T23:13:28.183+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T23:13:28.190+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T23:13:28.191+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T18:13:43.868+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T18:13:43.878+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T18:13:43.881+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T18:13:43.881+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T18:13:43.885+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-28T18:13:43.887+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-28T18:13:43.888+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpnvs4dy8o']
[2024-06-28T18:13:43.889+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-28T18:13:43.905+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 08cefd381a1f
[2024-06-28T18:13:43.933+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-28T18:13:43.934+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T18:13:43.989+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T18:13:43.989+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T18:13:43.989+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T18:13:43.989+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T18:13:43.990+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T18:13:43.994+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240628T181343, end_date=20240628T181343
[2024-06-28T18:13:44.043+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T18:13:44.050+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T18:13:44.051+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T18:20:59.620+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T18:20:59.630+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T18:20:59.633+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T18:20:59.633+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T18:20:59.637+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-28T18:20:59.639+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-28T18:20:59.640+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpw1sw7nd8']
[2024-06-28T18:20:59.640+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-28T18:20:59.657+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 6570e5857071
[2024-06-28T18:20:59.685+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-28T18:20:59.685+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T18:20:59.742+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T18:20:59.742+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T18:20:59.742+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T18:20:59.742+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T18:20:59.742+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T18:20:59.747+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240628T182059, end_date=20240628T182059
[2024-06-28T18:20:59.784+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T18:20:59.791+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T18:20:59.791+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T18:31:02.170+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T18:31:02.188+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T18:31:02.192+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T18:31:02.193+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T18:31:02.202+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-28T18:31:02.205+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-28T18:31:02.207+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmppzchzeop']
[2024-06-28T18:31:02.209+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-28T18:31:02.254+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 50bf780463b8
[2024-06-28T18:31:02.293+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-28T18:31:02.294+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T18:31:02.358+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T18:31:02.358+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T18:31:02.358+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T18:31:02.359+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T18:31:02.359+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T18:31:02.365+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240628T183102, end_date=20240628T183102
[2024-06-28T18:31:02.402+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T18:31:02.409+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T18:31:02.410+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T22:14:11.728+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T22:14:11.738+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T22:14:11.740+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-28T22:14:11.740+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T22:14:11.745+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-28T22:14:11.746+0000] {standard_task_runner.py:63} INFO - Started process 458 to run task
[2024-06-28T22:14:11.747+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp6rndpf0i']
[2024-06-28T22:14:11.748+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask generate_and_insert_data
[2024-06-28T22:14:11.765+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 619125b6796e
[2024-06-28T22:14:11.795+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-28T22:14:11.796+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T22:14:11.850+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T22:14:11.850+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T22:14:11.850+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T22:14:11.851+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T22:14:11.851+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T22:14:11.855+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240628T221411, end_date=20240628T221411
[2024-06-28T22:14:11.889+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T22:14:11.895+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T22:14:11.896+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-29T21:37:04.196+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-29T21:37:04.206+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-29T21:37:04.209+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-29T21:37:04.209+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-29T21:37:04.214+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-29T21:37:04.216+0000] {standard_task_runner.py:63} INFO - Started process 260 to run task
[2024-06-29T21:37:04.217+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp5qhk2sa5']
[2024-06-29T21:37:04.218+0000] {standard_task_runner.py:91} INFO - Job 17: Subtask generate_and_insert_data
[2024-06-29T21:37:04.236+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host afcb6fbf9265
[2024-06-29T21:37:04.266+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-29T21:37:04.267+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-29T21:37:04.321+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-29T21:37:04.321+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-29T21:37:04.322+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-29T21:37:04.322+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-29T21:37:04.322+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-29T21:37:04.327+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240629T213704, end_date=20240629T213704
[2024-06-29T21:37:04.358+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-29T21:37:04.365+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-29T21:37:04.365+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-29T21:49:20.896+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-29T21:49:20.907+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-29T21:49:20.909+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-29T21:49:20.909+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-29T21:49:20.914+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-29T21:49:20.915+0000] {standard_task_runner.py:63} INFO - Started process 261 to run task
[2024-06-29T21:49:20.916+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpj_uaeegw']
[2024-06-29T21:49:20.917+0000] {standard_task_runner.py:91} INFO - Job 17: Subtask generate_and_insert_data
[2024-06-29T21:49:20.933+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 0b7af056b90c
[2024-06-29T21:49:20.960+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-29T21:49:20.961+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-29T21:49:21.016+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-29T21:49:21.016+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-29T21:49:21.017+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-29T21:49:21.017+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-29T21:49:21.017+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-29T21:49:21.022+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240629T214920, end_date=20240629T214921
[2024-06-29T21:49:21.064+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-29T21:49:21.072+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-29T21:49:21.072+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-29T22:00:53.360+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-29T22:00:53.370+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-29T22:00:53.373+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [queued]>
[2024-06-29T22:00:53.373+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-29T22:00:53.377+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 02:30:00+00:00
[2024-06-29T22:00:53.379+0000] {standard_task_runner.py:63} INFO - Started process 261 to run task
[2024-06-29T22:00:53.380+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T02:30:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpl9pjmbcn']
[2024-06-29T22:00:53.381+0000] {standard_task_runner.py:91} INFO - Job 17: Subtask generate_and_insert_data
[2024-06-29T22:00:53.397+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T02:30:00+00:00 [running]> on host 799998b0d26e
[2024-06-29T22:00:53.426+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T02:30:00+00:00'
[2024-06-29T22:00:53.427+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-29T22:00:53.486+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-29T22:00:53.486+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-29T22:00:53.486+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-29T22:00:53.486+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-29T22:00:53.486+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-29T22:00:53.492+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T02:30:00+00:00, execution_date=20230101T023000, start_date=20240629T220053, end_date=20240629T220053
[2024-06-29T22:00:53.528+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-29T22:00:53.535+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-29T22:00:53.536+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
