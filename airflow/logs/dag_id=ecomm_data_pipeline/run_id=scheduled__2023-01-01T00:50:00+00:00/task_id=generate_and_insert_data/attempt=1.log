[2024-06-27T22:33:03.565+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T22:33:03.577+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T22:33:03.580+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T22:33:03.580+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T22:33:03.584+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-27T22:33:03.586+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-27T22:33:03.587+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpd01r6g6m']
[2024-06-27T22:33:03.588+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-27T22:33:03.605+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 9174587f6b1d
[2024-06-27T22:33:03.633+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-27T22:33:03.634+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T22:33:03.666+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', './generator/ecomm_generator.py']' returned non-zero exit status 2.
[2024-06-27T22:33:03.666+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T22:33:03.666+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T22:33:03.671+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240627T223303, end_date=20240627T223303
[2024-06-27T22:33:03.690+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T22:33:03.696+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T22:33:03.697+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T22:51:53.715+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T22:51:53.725+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T22:51:53.727+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T22:51:53.727+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T22:51:53.732+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-27T22:51:53.733+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-27T22:51:53.734+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpz73c_k3l']
[2024-06-27T22:51:53.735+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-27T22:51:53.751+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host a70edf9d8581
[2024-06-27T22:51:53.778+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-27T22:51:53.778+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T22:51:53.809+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', './generator/ecomm_generator.py']' returned non-zero exit status 2.
[2024-06-27T22:51:53.809+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T22:51:53.810+0000] {logging_mixin.py:188} INFO - stderr: python3: can't open file '/opt/airflow/./generator/ecomm_generator.py': [Errno 2] No such file or directory
[2024-06-27T22:51:53.810+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T22:51:53.810+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T22:51:53.814+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240627T225153, end_date=20240627T225153
[2024-06-27T22:51:53.839+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T22:51:53.845+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T22:51:53.846+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T22:59:37.174+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T22:59:37.183+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T22:59:37.186+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T22:59:37.186+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T22:59:37.190+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-27T22:59:37.191+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-27T22:59:37.192+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp9bfa2sc1']
[2024-06-27T22:59:37.193+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-27T22:59:37.209+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 5cc71d4b298c
[2024-06-27T22:59:37.236+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-27T22:59:37.237+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T22:59:37.291+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T22:59:37.291+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T22:59:37.292+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T22:59:37.292+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T22:59:37.292+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T22:59:37.297+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240627T225937, end_date=20240627T225937
[2024-06-27T22:59:37.347+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T22:59:37.354+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T22:59:37.355+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T23:09:16.473+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T23:09:16.483+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T23:09:16.485+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T23:09:16.486+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T23:09:16.490+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-27T23:09:16.491+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-27T23:09:16.493+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpwlylvfpp']
[2024-06-27T23:09:16.493+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-27T23:09:16.509+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 7147f8a4dc9f
[2024-06-27T23:09:16.536+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-27T23:09:16.537+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T23:09:16.593+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T23:09:16.594+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T23:09:16.594+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T23:09:16.594+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T23:09:16.595+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T23:09:16.601+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240627T230916, end_date=20240627T230916
[2024-06-27T23:09:16.631+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T23:09:16.638+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T23:09:16.639+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T23:12:54.491+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T23:12:54.500+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T23:12:54.503+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T23:12:54.503+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T23:12:54.507+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-27T23:12:54.509+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-27T23:12:54.510+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpfuunmjg5']
[2024-06-27T23:12:54.510+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-27T23:12:54.527+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 4bd55eb76a74
[2024-06-27T23:12:54.555+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-27T23:12:54.556+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T23:12:54.611+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T23:12:54.611+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T23:12:54.611+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T23:12:54.612+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T23:12:54.612+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T23:12:54.616+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240627T231254, end_date=20240627T231254
[2024-06-27T23:12:54.657+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T23:12:54.664+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T23:12:54.665+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-27T23:18:48.964+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T23:18:48.974+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T23:18:48.977+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-27T23:18:48.977+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-27T23:18:48.981+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-27T23:18:48.983+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-27T23:18:48.984+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp8p4cx0kx']
[2024-06-27T23:18:48.985+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-27T23:18:49.001+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 73964f870a02
[2024-06-27T23:18:49.031+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-27T23:18:49.031+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T23:18:49.090+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-27T23:18:49.090+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-27T23:18:49.090+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-27T23:18:49.090+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-27T23:18:49.091+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T23:18:49.096+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240627T231848, end_date=20240627T231849
[2024-06-27T23:18:49.132+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-27T23:18:49.139+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T23:18:49.139+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T18:13:11.966+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T18:13:11.976+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T18:13:11.978+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T18:13:11.978+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T18:13:11.983+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-28T18:13:11.985+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-28T18:13:11.986+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp96chy2fl']
[2024-06-28T18:13:11.986+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-28T18:13:12.003+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 08cefd381a1f
[2024-06-28T18:13:12.030+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-28T18:13:12.031+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T18:13:12.084+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T18:13:12.084+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T18:13:12.084+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T18:13:12.085+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T18:13:12.085+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T18:13:12.090+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240628T181311, end_date=20240628T181312
[2024-06-28T18:13:12.136+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T18:13:12.143+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T18:13:12.144+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T18:20:26.447+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T18:20:26.457+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T18:20:26.459+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T18:20:26.459+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T18:20:26.464+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-28T18:20:26.465+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-28T18:20:26.466+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpm84oj9li']
[2024-06-28T18:20:26.467+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-28T18:20:26.483+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 6570e5857071
[2024-06-28T18:20:26.510+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-28T18:20:26.510+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T18:20:26.566+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T18:20:26.567+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T18:20:26.567+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T18:20:26.567+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T18:20:26.567+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T18:20:26.572+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240628T182026, end_date=20240628T182026
[2024-06-28T18:20:26.613+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T18:20:26.620+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T18:20:26.620+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T18:30:28.362+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T18:30:28.371+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T18:30:28.374+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T18:30:28.374+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T18:30:28.378+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-28T18:30:28.380+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-28T18:30:28.381+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp9jt4pj7j']
[2024-06-28T18:30:28.381+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-28T18:30:28.397+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 50bf780463b8
[2024-06-28T18:30:28.424+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-28T18:30:28.424+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T18:30:28.480+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T18:30:28.480+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T18:30:28.480+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T18:30:28.480+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T18:30:28.481+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T18:30:28.485+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240628T183028, end_date=20240628T183028
[2024-06-28T18:30:28.523+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T18:30:28.530+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T18:30:28.530+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-28T22:13:38.215+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-28T22:13:38.225+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T22:13:38.227+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-28T22:13:38.227+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-28T22:13:38.231+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-28T22:13:38.233+0000] {standard_task_runner.py:63} INFO - Started process 178 to run task
[2024-06-28T22:13:38.234+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpwdvs8t7o']
[2024-06-28T22:13:38.235+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask generate_and_insert_data
[2024-06-28T22:13:38.252+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 619125b6796e
[2024-06-28T22:13:38.279+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-28T22:13:38.280+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-28T22:13:38.334+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-28T22:13:38.334+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-28T22:13:38.334+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-28T22:13:38.334+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-28T22:13:38.334+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-28T22:13:38.339+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240628T221338, end_date=20240628T221338
[2024-06-28T22:13:38.381+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-28T22:13:38.388+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-28T22:13:38.389+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-29T21:36:45.112+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-29T21:36:45.121+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-29T21:36:45.124+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-29T21:36:45.124+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-29T21:36:45.128+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-29T21:36:45.130+0000] {standard_task_runner.py:63} INFO - Started process 111 to run task
[2024-06-29T21:36:45.131+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpqfg2o579']
[2024-06-29T21:36:45.132+0000] {standard_task_runner.py:91} INFO - Job 7: Subtask generate_and_insert_data
[2024-06-29T21:36:45.148+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host afcb6fbf9265
[2024-06-29T21:36:45.175+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-29T21:36:45.176+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-29T21:36:45.230+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-29T21:36:45.230+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-29T21:36:45.230+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-29T21:36:45.230+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-29T21:36:45.230+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-29T21:36:45.235+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240629T213645, end_date=20240629T213645
[2024-06-29T21:36:45.275+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-29T21:36:45.282+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-29T21:36:45.283+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-29T21:49:01.730+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-29T21:49:01.740+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-29T21:49:01.743+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-29T21:49:01.743+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-29T21:49:01.747+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-29T21:49:01.748+0000] {standard_task_runner.py:63} INFO - Started process 112 to run task
[2024-06-29T21:49:01.749+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmpxqui2nrp']
[2024-06-29T21:49:01.750+0000] {standard_task_runner.py:91} INFO - Job 7: Subtask generate_and_insert_data
[2024-06-29T21:49:01.766+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 0b7af056b90c
[2024-06-29T21:49:01.793+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-29T21:49:01.794+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-29T21:49:01.849+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-29T21:49:01.850+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-29T21:49:01.850+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-29T21:49:01.850+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-29T21:49:01.850+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-29T21:49:01.855+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240629T214901, end_date=20240629T214901
[2024-06-29T21:49:01.896+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-29T21:49:01.903+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-29T21:49:01.903+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-29T22:00:34.258+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-29T22:00:34.269+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-29T22:00:34.271+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [queued]>
[2024-06-29T22:00:34.271+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-29T22:00:34.276+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): generate_and_insert_data> on 2023-01-01 00:50:00+00:00
[2024-06-29T22:00:34.277+0000] {standard_task_runner.py:63} INFO - Started process 112 to run task
[2024-06-29T22:00:34.278+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ecomm_data_pipeline', 'generate_and_insert_data', 'scheduled__2023-01-01T00:50:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/ecomm_dag.py', '--cfg-path', '/tmp/tmp62x0v5cc']
[2024-06-29T22:00:34.279+0000] {standard_task_runner.py:91} INFO - Job 7: Subtask generate_and_insert_data
[2024-06-29T22:00:34.296+0000] {task_command.py:426} INFO - Running <TaskInstance: ecomm_data_pipeline.generate_and_insert_data scheduled__2023-01-01T00:50:00+00:00 [running]> on host 799998b0d26e
[2024-06-29T22:00:34.323+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ecomm_data_pipeline' AIRFLOW_CTX_TASK_ID='generate_and_insert_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-01-01T00:50:00+00:00'
[2024-06-29T22:00:34.323+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-29T22:00:34.377+0000] {logging_mixin.py:188} INFO - Error in generate_and_insert_data: Command '['python3', '/opt/airflow/generator/ecomm_generator.py']' returned non-zero exit status 1.
[2024-06-29T22:00:34.377+0000] {logging_mixin.py:188} INFO - stdout: 
[2024-06-29T22:00:34.378+0000] {logging_mixin.py:188} INFO - stderr: Traceback (most recent call last):
  File "/opt/airflow/generator/ecomm_generator.py", line 4, in <module>
    from faker import Faker
ModuleNotFoundError: No module named 'faker'
[2024-06-29T22:00:34.378+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-29T22:00:34.378+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-29T22:00:34.382+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ecomm_data_pipeline, task_id=generate_and_insert_data, run_id=scheduled__2023-01-01T00:50:00+00:00, execution_date=20230101T005000, start_date=20240629T220034, end_date=20240629T220034
[2024-06-29T22:00:34.423+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-29T22:00:34.431+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-29T22:00:34.431+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
