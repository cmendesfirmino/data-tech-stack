services:
  postgres_data:
    image: postgres:15
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./pgdata:/var/lib/postgresql/data
      - ./init-db:/docker-entrypoint-initdb.d/
    command: ["/bin/bash", "-c", "echo \"-- Create databases

                CREATE DATABASE airflow;
                CREATE DATABASE superset;
                CREATE DATABASE warehouse;

                -- Create users with passwords

                CREATE USER ${AIRFLOW_WWW_USER_USERNAME} WITH PASSWORD '${AIRFLOW_WWW_USER_PASSWORD}';
                CREATE USER ${SUPERSET_ADMIN} WITH PASSWORD '${SUPERSET_PASSWORD}';
                CREATE USER ${DW_USER} WITH PASSWORD '${DW_PASSWORD}';

                -- Grant privileges to users on their respective databases

                GRANT ALL PRIVILEGES ON DATABASE airflow TO ${AIRFLOW_WWW_USER_USERNAME};
                GRANT ALL PRIVILEGES ON DATABASE superset TO ${SUPERSET_ADMIN};
                GRANT ALL PRIVILEGES ON DATABASE warehouse TO ${DW_USER};
                \" > /docker-entrypoint-initdb.d/init.sql && docker-entrypoint.sh postgres"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d postgres"]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s
    restart: always


  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping"]
      interval: 30s
      timeout: 30s
      retries: 3

  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/warehouse
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
    entrypoint: ["/entrypoint.sh", "db", "init"]
    depends_on:
      postgres_data:
        condition: service_healthy
      redis:
        condition: service_healthy

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/warehouse
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      AIRFLOW__CORE__LOGGING_LEVEL: "INFO"
      AIRFLOW__WEBSERVER__BASE_URL: "http://localhost:8080"
      AIRFLOW__LOGGING__LOG_URL: "http://localhost:8080/log/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}.log"
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./data:/opt/airflow/data
    command: ["scheduler"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 10s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - default

  airflow-worker:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/warehouse
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./data:/opt/airflow/data
    healthcheck:
      test: ["CMD-SHELL", "python /opt/airflow/scripts/worker_health_check.py"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 10s
    command: >
      bash -c "
      airflow celery worker --concurrency=1
      "
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      airflow-scheduler:
        condition: service_healthy
    networks:
      - default
  
  airflow:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/warehouse
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__LOGGING_LEVEL: "INFO"
      AIRFLOW__WEBSERVER__BASE_URL: "http://localhost:8080"
      AIRFLOW__LOGGING__LOG_URL: "http://localhost:8080/log/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}.log"
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./data:/opt/airflow/data
    ports:
      - "8080:8080"
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username ${AIRFLOW_WWW_USER_USERNAME} --password ${AIRFLOW_WWW_USER_PASSWORD} --firstname FIRSTNAME --lastname LASTNAME --role Admin --email admin@example.com &&
      exec airflow webserver"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-scheduler:
        condition: service_healthy
      airflow-worker:
        condition: service_healthy
      redis:
        condition: service_healthy


  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    environment:
      SUPERSET_ADMIN: ${SUPERSET_ADMIN}
      SUPERSET_PASSWORD: ${SUPERSET_PASSWORD}
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_data:5432/superset
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
    command: /app/init_superset.sh
    depends_on:
      - postgres_data

networks:
  default:
    driver: bridge

# volumes:
#   superset_db_data:
#   superset_home:
#   company_dw_data:

volumes:
  postgres_data:
  superset_home: